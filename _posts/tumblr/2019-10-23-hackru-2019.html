---
layout: post
title: HackRU 2019
date: '2019-10-23T22:30:07-04:00'
tags:
- hackathon
tumblr_url: https://boxfishdev.tumblr.com/post/188552233394/hackru-2019
---
<figure data-orig-width="1424" data-orig-height="682" class="tmblr-full"><img src="https://66.media.tumblr.com/1395f4690ac7dcd827c8d45ac1235cd6/323601978c93fc48-41/s540x810/38fd4a7799f8b9b7a56156ef6fa817d62195f9da.png" alt="image" data-orig-width="1424" data-orig-height="682"/></figure><p>Over the weekend, a friend and I went down to New Jersey to compete in HackRU, Rutgers’ main hackathon. Normally, I wouldn’t enter a hackathon that takes ~9 hours to get to, but I basically grew up next to Rutgers, so it was a chance for me to come home for a little bit. Since it’s so close to where I live, I’ve wanted to go to HackRU since I first heard about it in high school. Not gonna lie, going there after all this time, I was a little disappointed. It wasn’t bad by any means, but I’ve seen better opening ceremonies, and I’m pretty sure only one tech company sponsored and showed up. It’s not like NJ has a thriving tech sector, though, and it <b>is</b> midterm season, so I guess I can’t fault them too much.</p><p>Before we even boarded the bus to NJ, my friend and I spitballed some ideas. We decided to go with a web app where users could upload videos, then dance next to them. If this sounds really familiar, it’s pretty much what you think it is. The difference between this and TikTok, though, is when you dance next to the video, at the end, you get a score of how well you mimicked the dancer in the video. By the time we finished, what we ended up with was Mimic.</p><p>We (rightfully) predicted that, since we’d be braindead in the latter half of the competition, we should figure out how the cloud services we use and our code should be linked up before we start. Here’s what we decided on:</p><figure class="tmblr-full" data-orig-height="505" data-orig-width="1431"><img src="https://66.media.tumblr.com/f90cb9b7fedc58c1ccf99ff17daae8e1/323601978c93fc48-50/s540x810/4330a7ccc1397d49211447d6158ee5f1d4e64b1e.jpg" data-orig-height="505" data-orig-width="1431"/></figure><p>Let’s break it down in human language. Our infrastructure made use of three Google Cloud Services:</p><ul><li>AppEngine: Hosts our main web app and backend.</li><li>CloudSQL: Stores video related data.</li><li>Cloud Storage: Stores the videos themselves.</li></ul><div><p>A Google AI service was also gonna be used to do handle our video analysis, but we decided to just host it on AppEngine instead along with our web app.</p><p>On the left is our video processing flow. When a user uploads a video to dance to, our backend breaks it down into a bunch of frames. Then, each frame gets fed into an AI model that spits out the location of hands, arms, feet, etc. Using that data, we can generate a “skeleton” of the dancer over the course of the video. The skeleton data and video gets saved in Google Cloud storage, while the location of both files gets saved in the database for easy querying.</p><p>On the right is our video grading flow. When the user selects which video they want to dance to, the database tells our backend which files to queue up. The webapp then displays two video streams: the video we’ve already processed, and the output of the user’s webcam. Our processed video plays, and once it’s finished, a video of the user dancing gets sent to our backend. From there, our AI processes it to look for poses, then skeletons from both videos are compared and the score is sent to the webapp.</p><p>While my partner handled the main webapp, I wrote the AI. Or at least tried to. I first needed to do some research on how to get this done in the first place, so I looked up how to analyze pictures for poses. That lead me to <a href="https://nanonets.com/blog/human-pose-estimation-2d-guide/">this link</a>. From what I could gather, a popular technique is using convolutional neural networks to generate a “heatmap” of likely limb positions, then generating the skeleton off of that. Armed with my shaky knowledge of how to do this, I then went on a hunt for some pose data. The good news: I quickly found a nice resource, namely the MPII dataset. The bad news: each dataset piece is like 5 gigabytes, and there was <b>NO</b> way I was gonna download it and train my model in time. After wasting a bunch of time, I had to use a pretrained model from <a href="https://gluon-cv.mxnet.io/">GluonCV</a>. I should have done more research, though, because apparently Tensorflow has models that do pose estimation, and I would choose something built in Tensorflow over anything else in a heartbeat.</p><p>My partner also had a couple difficulties. Namely, uploading a video to a server in Node JS is apparently such a difficult task, the Internet doesn’t have a clear answer on how to do it. At one point, we even considered just having the frontend directly save to our cloud storage, but that would be crazy. Eventually, we rewrote everything in Flask, and that’s when we started making some progress. As always, someone smarter than us already created a library that did exactly what we needed to do, so we just imported it in and all was good.</p><p>In the end, we didn’t quite finish. I had trouble connecting the AI to our video files, so we just generated a random score after the video ended to get our demo to work. The one other thing I really wish we put in is some kind of countdown before the videos started playing. It was pretty awkward being like “check this out!”, then, with little fanfare, playing the default dance and a webcam stream of some sleep deprived hackers and a very confused judge.<br/></p></div>
